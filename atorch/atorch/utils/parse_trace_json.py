# coding=utf-8
"""
parse torch profile json file
feature:
1. analyze gpu kernel time
2. analyze communicate overlap,get no overlap kernel
"""
from __future__ import absolute_import, unicode_literals

from argparse import ArgumentParser
from json import load

import numpy as np
import pandas as pd


def get_compute_kernel(df):
    return df.query("cat == 'kernel' and ~name.str.startswith('nccl')")


def prepare_df(json_obj):
    traceEvents = json_obj["traceEvents"]
    for traceEvent in traceEvents:
        if "cat" in traceEvent:
            traceEvent["cat"] = traceEvent["cat"].lower()
        if "dur" in traceEvent:
            traceEvent["dur"] = int(traceEvent["dur"])
        else:
            traceEvent["dur"] = 0
        if "ts" in traceEvent:
            traceEvent["ts"] = int(traceEvent["ts"])
        if "ts" in traceEvent and "dur" in traceEvent:
            traceEvent["finish_time"] = traceEvent["ts"] + traceEvent["dur"]
    df = pd.DataFrame(traceEvents)
    return df


def analyze_gpu_kernel(df, verbose=False):
    # diff torch version have diff cat name
    # TODO: split gpu kernel with H2D and D2H
    df["cat"] = df["cat"]
    gpu_kernel_df = get_compute_kernel(df)
    all_compute_tids = list(set(gpu_kernel_df["tid"].values))
    if verbose:
        print("all_compute_tids", all_compute_tids)

    if gpu_kernel_df.empty:
        return {"op_cat_times": {"gemm": 0, "elementwise": 0, "other": 0}, "gpu_time_us": 0, "total_gpu_rate": 0}
    gpu_kernel_start = gpu_kernel_df["ts"].min()
    gpu_kernel_start_end = gpu_kernel_df["finish_time"].max()
    # how many gpu time cost from start to end
    gpu_time = gpu_kernel_df["dur"].sum()
    total_gpu_rate = (gpu_time) / (gpu_kernel_start_end - gpu_kernel_start)
    # gemm、elementwise
    # consider gpu kernel will overlap
    op_times = {
        "gemm": gpu_kernel_df[gpu_kernel_df.name.str.contains("gemm")]["dur"].sum(),
        "elementwise": gpu_kernel_df[gpu_kernel_df.name.str.contains("elementwise")]["dur"].sum(),
        "layernorm": gpu_kernel_df[
            gpu_kernel_df.name.str.contains("cuApplyLayerNorm")  # forward
            ^ gpu_kernel_df.name.str.contains("cuComputeGradInput")  # backward
        ]["dur"].sum(),
        "other": gpu_kernel_df[
            ~(
                gpu_kernel_df.name.str.contains("elementwise")
                ^ gpu_kernel_df.name.str.contains("gemm")
                ^ gpu_kernel_df.name.str.contains("cuApplyLayerNorm")
            )
        ]["dur"].sum(),
    }
    return {"op_cat_times": op_times, "gpu_time_us": gpu_time, "total_gpu_rate": total_gpu_rate}


def show_compute_comm_ts(compute_op, comm_op):
    delta = comm_op.ts
    return "comm start=%s end=%s \n compute start=%s end=%s" % (
        comm_op.ts - delta,
        comm_op.finish_time - delta,
        compute_op.ts - delta,
        compute_op.finish_time - delta,
    )


def fused_kernels(kernels):
    # fused time: A: 1-3, B: 2-4, fused: 1-4; so we can compute overlap time exactly
    kernels = sorted(kernels, key=lambda x: x.ts)
    fused_kernel = []
    current_kernel = None
    for kernel in kernels:
        if current_kernel is None:
            current_kernel = kernel
        else:
            if current_kernel.finish_time >= kernel.ts:
                current_kernel.finish_time = max(current_kernel.finish_time, kernel.finish_time)
            else:
                fused_kernel.append(current_kernel)
                current_kernel = kernel
    if current_kernel is not None:
        fused_kernel.append(current_kernel)
    return fused_kernel


def analyze_communicate_overlap(df, verbose=False):
    all_comm_tids = list(set(df.query("name.str.startswith('ncclKernel')")["tid"].values))
    all_compute_tids = list(set(df.query("~name.str.startswith('nccl')")["tid"].values))
    if verbose:
        print("all_comm_tids", all_comm_tids)
        print("all_compute_tids", all_compute_tids)
    gpu_kernel_df = get_compute_kernel(df)
    # query all communicate kernel which tid in list
    comm_kernel_df = df.query("name.str.startswith('ncclKernel')")
    if comm_kernel_df.empty:
        return {
            "overlap_rate": 0,
            "comm_time_us": 0,
            "overlap_time_us": 0,
            "nooverlap_comm_df": comm_kernel_df,
        }
    comm_time_us = comm_kernel_df["dur"].sum()
    # int64 = int64 + int64, can dur convert to int64?
    # gpu_kernel_df.loc[:,"finish_time"] = gpu_kernel_df["ts"] + gpu_kernel_df["dur"]
    # comm_kernel_df.loc[:,"finish_time"] = comm_kernel_df["ts"] + comm_kernel_df["dur"]
    # try to reduce query time
    # how many communicate time overlap with compute
    overlap_time = 0
    comm_kernels = [comm_kernel for _, comm_kernel in comm_kernel_df.iterrows()]
    compute_kernels = [compute_kernel for _, compute_kernel in gpu_kernel_df.iterrows()]
    comm_kernels = fused_kernels(comm_kernels)
    compute_kernels = fused_kernels(compute_kernels)
    print("num of comm kernels", len(comm_kernels))
    compute_idx = 0
    comm_idx = 0
    compute_length = len(compute_kernels)
    comm_length = len(comm_kernels)
    current_comm_overlap_time = 0
    idx2overlap_time = {}
    # TODO: there is no consider computer kernel have multi stream
    while compute_idx < compute_length and comm_idx < comm_length:
        current_compute_op = compute_kernels[compute_idx]

        current_comm_op = comm_kernels[comm_idx]
        # skip if they have no overlap
        if current_compute_op.finish_time < current_comm_op.ts:
            compute_idx += 1
            continue
        elif current_compute_op.ts > current_comm_op.finish_time:
            idx2overlap_time[comm_idx] = current_comm_overlap_time / current_comm_op.dur
            comm_idx += 1
            current_comm_overlap_time = 0
            continue

        if current_compute_op.ts <= current_comm_op.ts:
            if current_compute_op.finish_time <= current_comm_op.finish_time:
                overlap_time += current_compute_op.finish_time - current_comm_op.ts
                current_comm_overlap_time += current_compute_op.finish_time - current_comm_op.ts
                compute_idx += 1
            else:
                overlap_time += current_comm_op.dur
                current_comm_overlap_time += current_comm_op.dur
                idx2overlap_time[comm_idx] = current_comm_overlap_time / current_comm_op.dur

                comm_idx += 1
                current_comm_overlap_time = 0
        else:
            if current_compute_op.finish_time <= current_comm_op.finish_time:
                overlap_time += current_compute_op["dur"]
                current_comm_overlap_time += current_compute_op["dur"]
                compute_idx += 1

            else:

                overlap_time += current_comm_op.finish_time - current_compute_op.ts
                current_comm_overlap_time += current_comm_op.finish_time - current_compute_op.ts

                comm_idx += 1
                idx2overlap_time[comm_idx] = current_comm_overlap_time / current_comm_op.dur
                current_comm_overlap_time = 0

    idx_overlap_list = list(idx2overlap_time.items())
    topdown_idx2overlap_time = sorted(idx_overlap_list, key=lambda x: x[1])
    df_data = []
    length_of_comm = len(comm_kernels)
    for idx, overlap_rate in topdown_idx2overlap_time:
        if idx >= length_of_comm:
            break
        kernel = comm_kernels[idx]
        df_data.append(
            {
                "idx": idx,
                "id": kernel.id,
                "kernel_ts": kernel.ts,
                "dur": kernel.dur,
                "name": kernel["name"],
                "overlap_rate": overlap_rate,
            }
        )
    comm_df = pd.DataFrame(df_data)
    if comm_df.empty:
        return {
            "overlap_rate": 0,
            "comm_time_us": comm_time_us,
            "overlap_time_us": overlap_time,
            "nooverlap_comm_df": comm_df,
        }
    comm_df["no_overlap_time_us"] = comm_df["dur"] * (1 - comm_df["overlap_rate"])

    nooverlap_comm_df = comm_df.sort_values(["no_overlap_time_us", "dur"], ascending=[False, False]).head(n=10)
    return {
        "overlap_rate": overlap_time / comm_time_us,
        "comm_time_us": comm_time_us,
        "overlap_time_us": overlap_time,
        "nooverlap_comm_df": nooverlap_comm_df,
    }


def main():
    parser = ArgumentParser(usage="""python parse_trace_json.py trace_1.json""")
    parser.add_argument("json_files", nargs="*")
    parser.add_argument("--verbose", "-v", action="store_true")
    args = parser.parse_args()
    kernel_start_times = []
    for json_file in args.json_files:
        with open(json_file, "r") as fin:
            json_obj = load(fin)  # TODO: iter json_obj,save memory usage
            df = prepare_df(json_obj)
            kernel_start_times.append(df.query("cat=='kernel'").head(1)["ts"].values[0])
            print("kernel 5 sample:\n", df.query("cat=='kernel'").head(n=5))

            ret = analyze_gpu_kernel(df)
            print("compute summary:", ret)
            ret = analyze_communicate_overlap(df, args.verbose)
            nooverlap_comm_df = ret.pop("nooverlap_comm_df")
            print("communicate summary:", ret)
            print("no overlap comm op:\n", nooverlap_comm_df)
    # check if all kernels start at the same time
    kernel_start_times = np.asarray(kernel_start_times)
    min_start_time = np.min(kernel_start_times)
    print(kernel_start_times - min_start_time)


if __name__ == "__main__":
    main()
